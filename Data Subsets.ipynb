{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 16893 × 6998\n",
      "    obs: 'condition', 'n_counts', 'n_genes', 'mt_frac', 'cell_type'\n",
      "    var: 'gene_symbol', 'n_cells'\n",
      "    uns: 'cell_type_colors', 'condition_colors', 'neighbors'\n",
      "    obsm: 'X_pca', 'X_tsne', 'X_umap'\n",
      "    obsp: 'connectivities', 'distances'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import anndata as ad\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "\n",
    "from anndata import AnnData\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initialize constants\n",
    "load_dotenv()\n",
    "CONDITION_KEY, CELL_TYPE_KEY = os.getenv('CONDITION_KEY'), os.getenv('CELL_TYPE_KEY')\n",
    "\n",
    "from load_data import get_adata\n",
    "from sc_condition_prediction import create_and_train_vae_model, evaluate_r2, N_INPUT, N_LAYERS, N_HIDDEN, N_LATENT, BATCH_SIZE\n",
    "from utils import remove_stimulated_for_celltype\n",
    "\n",
    "# Load data\n",
    "train_adata = get_adata(train=True, verbose=True)\n",
    "train_adata_no_cd4t = remove_stimulated_for_celltype(train_adata, celltype=\"CD4T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>index</th>\n",
       "      <th>AL627309.1</th>\n",
       "      <th>RP11-206L10.9</th>\n",
       "      <th>LINC00115</th>\n",
       "      <th>NOC2L</th>\n",
       "      <th>KLHL17</th>\n",
       "      <th>HES4</th>\n",
       "      <th>ISG15</th>\n",
       "      <th>TNFRSF18</th>\n",
       "      <th>TNFRSF4</th>\n",
       "      <th>SDF4</th>\n",
       "      <th>...</th>\n",
       "      <th>C21orf67</th>\n",
       "      <th>FAM207A</th>\n",
       "      <th>ADARB1</th>\n",
       "      <th>POFUT2</th>\n",
       "      <th>COL18A1</th>\n",
       "      <th>SLC19A1</th>\n",
       "      <th>COL6A2</th>\n",
       "      <th>FTCD</th>\n",
       "      <th>DIP2A</th>\n",
       "      <th>S100B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16893.000000</td>\n",
       "      <td>16893.000000</td>\n",
       "      <td>16893.000000</td>\n",
       "      <td>16893.000000</td>\n",
       "      <td>16893.000000</td>\n",
       "      <td>16893.000000</td>\n",
       "      <td>16893.000000</td>\n",
       "      <td>16893.000000</td>\n",
       "      <td>16893.000000</td>\n",
       "      <td>16893.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>16893.000000</td>\n",
       "      <td>16893.000000</td>\n",
       "      <td>16893.000000</td>\n",
       "      <td>16893.000000</td>\n",
       "      <td>16893.000000</td>\n",
       "      <td>16893.000000</td>\n",
       "      <td>16893.000000</td>\n",
       "      <td>16893.000000</td>\n",
       "      <td>16893.000000</td>\n",
       "      <td>16893.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.056011</td>\n",
       "      <td>0.000991</td>\n",
       "      <td>0.114630</td>\n",
       "      <td>1.799000</td>\n",
       "      <td>0.053575</td>\n",
       "      <td>0.052044</td>\n",
       "      <td>0.067051</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.032279</td>\n",
       "      <td>0.003885</td>\n",
       "      <td>0.006159</td>\n",
       "      <td>0.004496</td>\n",
       "      <td>0.002654</td>\n",
       "      <td>0.001596</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.013051</td>\n",
       "      <td>0.009183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.012740</td>\n",
       "      <td>0.018630</td>\n",
       "      <td>0.050151</td>\n",
       "      <td>0.204129</td>\n",
       "      <td>0.027527</td>\n",
       "      <td>0.316232</td>\n",
       "      <td>1.666201</td>\n",
       "      <td>0.230176</td>\n",
       "      <td>0.226953</td>\n",
       "      <td>0.216867</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017223</td>\n",
       "      <td>0.154578</td>\n",
       "      <td>0.054166</td>\n",
       "      <td>0.067945</td>\n",
       "      <td>0.059533</td>\n",
       "      <td>0.046549</td>\n",
       "      <td>0.036505</td>\n",
       "      <td>0.011595</td>\n",
       "      <td>0.099866</td>\n",
       "      <td>0.098072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.736927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.120183</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.052949</td>\n",
       "      <td>1.027135</td>\n",
       "      <td>2.020838</td>\n",
       "      <td>2.251955</td>\n",
       "      <td>1.069814</td>\n",
       "      <td>2.507464</td>\n",
       "      <td>5.472819</td>\n",
       "      <td>2.349544</td>\n",
       "      <td>2.472356</td>\n",
       "      <td>2.879868</td>\n",
       "      <td>...</td>\n",
       "      <td>1.468089</td>\n",
       "      <td>1.657621</td>\n",
       "      <td>2.230935</td>\n",
       "      <td>1.538911</td>\n",
       "      <td>1.213284</td>\n",
       "      <td>1.514670</td>\n",
       "      <td>1.494599</td>\n",
       "      <td>0.996147</td>\n",
       "      <td>1.607041</td>\n",
       "      <td>2.438123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 6998 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "index    AL627309.1  RP11-206L10.9     LINC00115         NOC2L        KLHL17  \\\n",
       "count  16893.000000   16893.000000  16893.000000  16893.000000  16893.000000   \n",
       "mean       0.000203       0.000442      0.003300      0.056011      0.000991   \n",
       "std        0.012740       0.018630      0.050151      0.204129      0.027527   \n",
       "min        0.000000       0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000       0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000       0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000       0.000000      0.000000      0.000000      0.000000   \n",
       "max        1.052949       1.027135      2.020838      2.251955      1.069814   \n",
       "\n",
       "index          HES4         ISG15      TNFRSF18       TNFRSF4          SDF4  \\\n",
       "count  16893.000000  16893.000000  16893.000000  16893.000000  16893.000000   \n",
       "mean       0.114630      1.799000      0.053575      0.052044      0.067051   \n",
       "std        0.316232      1.666201      0.230176      0.226953      0.216867   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      1.736927      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      3.120183      0.000000      0.000000      0.000000   \n",
       "max        2.507464      5.472819      2.349544      2.472356      2.879868   \n",
       "\n",
       "index  ...      C21orf67       FAM207A        ADARB1        POFUT2  \\\n",
       "count  ...  16893.000000  16893.000000  16893.000000  16893.000000   \n",
       "mean   ...      0.000333      0.032279      0.003885      0.006159   \n",
       "std    ...      0.017223      0.154578      0.054166      0.067945   \n",
       "min    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "25%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "50%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "75%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "max    ...      1.468089      1.657621      2.230935      1.538911   \n",
       "\n",
       "index       COL18A1       SLC19A1        COL6A2          FTCD         DIP2A  \\\n",
       "count  16893.000000  16893.000000  16893.000000  16893.000000  16893.000000   \n",
       "mean       0.004496      0.002654      0.001596      0.000152      0.013051   \n",
       "std        0.059533      0.046549      0.036505      0.011595      0.099866   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        1.213284      1.514670      1.494599      0.996147      1.607041   \n",
       "\n",
       "index         S100B  \n",
       "count  16893.000000  \n",
       "mean       0.009183  \n",
       "std        0.098072  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max        2.438123  \n",
       "\n",
       "[8 rows x 6998 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_adata.to_df().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_subsets_from_adata(adata: AnnData, verbose=False):\n",
    "    cell_types = adata.obs[CELL_TYPE_KEY].cat.categories.values\n",
    "    if verbose:\n",
    "        print(f\"Unique cell types: {cell_types}\")\n",
    "    adata_by_cell_type = [ \n",
    "        adata[adata.obs[CELL_TYPE_KEY] == cell_type]\n",
    "        for cell_type in cell_types\n",
    "    ]\n",
    "    if verbose:\n",
    "        print(f\"AnnData objects by cell types: {adata_by_cell_type}\")\n",
    "    from itertools import combinations\n",
    "    adata_subsets = []\n",
    "    for i in range(1, len(adata_by_cell_type) + 1):  # to get all subsets: from 1 to size (omitting 0 subset)\n",
    "        for adata_subset in combinations(adata_by_cell_type, i):\n",
    "            adata_subsets.append(adata_subset)\n",
    "    return adata_subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique cell types: ['CD4T' 'CD14+Mono' 'B' 'CD8T' 'NK' 'FCGR3A+Mono' 'Dendritic']\n",
      "AnnData objects by cell types: [View of AnnData object with n_obs × n_vars = 5564 × 6998\n",
      "    obs: 'condition', 'n_counts', 'n_genes', 'mt_frac', 'cell_type'\n",
      "    var: 'gene_symbol', 'n_cells'\n",
      "    uns: 'cell_type_colors', 'condition_colors', 'neighbors'\n",
      "    obsm: 'X_pca', 'X_tsne', 'X_umap'\n",
      "    obsp: 'connectivities', 'distances', View of AnnData object with n_obs × n_vars = 2561 × 6998\n",
      "    obs: 'condition', 'n_counts', 'n_genes', 'mt_frac', 'cell_type'\n",
      "    var: 'gene_symbol', 'n_cells'\n",
      "    uns: 'cell_type_colors', 'condition_colors', 'neighbors'\n",
      "    obsm: 'X_pca', 'X_tsne', 'X_umap'\n",
      "    obsp: 'connectivities', 'distances', View of AnnData object with n_obs × n_vars = 1811 × 6998\n",
      "    obs: 'condition', 'n_counts', 'n_genes', 'mt_frac', 'cell_type'\n",
      "    var: 'gene_symbol', 'n_cells'\n",
      "    uns: 'cell_type_colors', 'condition_colors', 'neighbors'\n",
      "    obsm: 'X_pca', 'X_tsne', 'X_umap'\n",
      "    obsp: 'connectivities', 'distances', View of AnnData object with n_obs × n_vars = 1115 × 6998\n",
      "    obs: 'condition', 'n_counts', 'n_genes', 'mt_frac', 'cell_type'\n",
      "    var: 'gene_symbol', 'n_cells'\n",
      "    uns: 'cell_type_colors', 'condition_colors', 'neighbors'\n",
      "    obsm: 'X_pca', 'X_tsne', 'X_umap'\n",
      "    obsp: 'connectivities', 'distances', View of AnnData object with n_obs × n_vars = 1163 × 6998\n",
      "    obs: 'condition', 'n_counts', 'n_genes', 'mt_frac', 'cell_type'\n",
      "    var: 'gene_symbol', 'n_cells'\n",
      "    uns: 'cell_type_colors', 'condition_colors', 'neighbors'\n",
      "    obsm: 'X_pca', 'X_tsne', 'X_umap'\n",
      "    obsp: 'connectivities', 'distances', View of AnnData object with n_obs × n_vars = 3601 × 6998\n",
      "    obs: 'condition', 'n_counts', 'n_genes', 'mt_frac', 'cell_type'\n",
      "    var: 'gene_symbol', 'n_cells'\n",
      "    uns: 'cell_type_colors', 'condition_colors', 'neighbors'\n",
      "    obsm: 'X_pca', 'X_tsne', 'X_umap'\n",
      "    obsp: 'connectivities', 'distances', View of AnnData object with n_obs × n_vars = 1078 × 6998\n",
      "    obs: 'condition', 'n_counts', 'n_genes', 'mt_frac', 'cell_type'\n",
      "    var: 'gene_symbol', 'n_cells'\n",
      "    uns: 'cell_type_colors', 'condition_colors', 'neighbors'\n",
      "    obsm: 'X_pca', 'X_tsne', 'X_umap'\n",
      "    obsp: 'connectivities', 'distances']\n"
     ]
    }
   ],
   "source": [
    "subsets_adata_by_cell_type = make_subsets_from_adata(train_adata, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected more than 1 value per channel when training, got input size torch.Size([1, 200])",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [9], line 13\u001B[0m\n\u001B[1;32m     11\u001B[0m adata_sample \u001B[38;5;241m=\u001B[39m ad\u001B[38;5;241m.\u001B[39mconcat(\u001B[38;5;28mlist\u001B[39m(subset_adata_by_cell_type), join\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mouter\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     12\u001B[0m params_filename \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(model_directory, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_autoencoder.pt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 13\u001B[0m \u001B[43mcreate_and_train_vae_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43madata_sample\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m15\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m                           \u001B[49m\u001B[43msave_params_to_filename\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams_filename\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m r2, r2_diff_genes \u001B[38;5;241m=\u001B[39m evaluate_r2(params_filename)\n\u001B[1;32m     17\u001B[0m df_results\u001B[38;5;241m.\u001B[39mloc[i, [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mr2\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mr2_diff_genes\u001B[39m\u001B[38;5;124m\"\u001B[39m]] \u001B[38;5;241m=\u001B[39m [r2, r2_diff_genes]\n",
      "File \u001B[0;32m~/Workspace/Education/University/Toronto/vaegen/sc_condition_prediction.py:74\u001B[0m, in \u001B[0;36mcreate_and_train_vae_model\u001B[0;34m(adata, epochs, save_params_to_filename)\u001B[0m\n\u001B[1;32m     69\u001B[0m autoencoder \u001B[38;5;241m=\u001B[39m VAE(n_input\u001B[38;5;241m=\u001B[39mN_INPUT, \n\u001B[1;32m     70\u001B[0m                   n_layers\u001B[38;5;241m=\u001B[39mN_LAYERS, \n\u001B[1;32m     71\u001B[0m                   n_hidden\u001B[38;5;241m=\u001B[39mN_HIDDEN, \n\u001B[1;32m     72\u001B[0m                   n_latent\u001B[38;5;241m=\u001B[39mN_LATENT)\n\u001B[1;32m     73\u001B[0m \u001B[38;5;66;03m# TRAIN\u001B[39;00m\n\u001B[0;32m---> 74\u001B[0m autoencoder \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mautoencoder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m     75\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mdataloader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m     76\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;66;03m# SAVE parameters\u001B[39;00m\n\u001B[1;32m     78\u001B[0m torch\u001B[38;5;241m.\u001B[39msave(autoencoder\u001B[38;5;241m.\u001B[39mstate_dict(), save_params_to_filename)\n",
      "File \u001B[0;32m~/Workspace/Education/University/Toronto/vaegen/sc_condition_prediction.py:45\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(autoencoder, dataloader, epochs, verbose)\u001B[0m\n\u001B[1;32m     43\u001B[0m opt\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     44\u001B[0m \u001B[38;5;66;03m# forward\u001B[39;00m\n\u001B[0;32m---> 45\u001B[0m qz_m, qz_v, z \u001B[38;5;241m=\u001B[39m \u001B[43mautoencoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     46\u001B[0m x_hat \u001B[38;5;241m=\u001B[39m autoencoder\u001B[38;5;241m.\u001B[39mdecoder(z)\n\u001B[1;32m     47\u001B[0m \u001B[38;5;66;03m# loss & backward\u001B[39;00m\n",
      "File \u001B[0;32m~/Workspace/Education/University/Toronto/vaegen/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Workspace/Education/University/Toronto/vaegen/VAE.py:44\u001B[0m, in \u001B[0;36mEncoder.forward\u001B[0;34m(self, x, *cat_list)\u001B[0m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, \u001B[38;5;241m*\u001B[39mcat_list):\n\u001B[0;32m---> 44\u001B[0m     q \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     45\u001B[0m     qz_m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmean_encoder(q)\n\u001B[1;32m     46\u001B[0m     qz_v \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvar_activation(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvar_encoder(q))  \u001B[38;5;66;03m# we often apply an activation function exp() on variation to ensure positivity (more: https://avandekleut.github.io/vae/)\u001B[39;00m\n",
      "File \u001B[0;32m~/Workspace/Education/University/Toronto/vaegen/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Workspace/Education/University/Toronto/vaegen/VAE.py:32\u001B[0m, in \u001B[0;36mFCLayers.forward\u001B[0;34m(self, *inputs)\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39minputs):\n\u001B[1;32m     31\u001B[0m     input_cat \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat(inputs, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfc\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_cat\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Workspace/Education/University/Toronto/vaegen/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Workspace/Education/University/Toronto/vaegen/venv/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    202\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    203\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 204\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    205\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/Workspace/Education/University/Toronto/vaegen/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Workspace/Education/University/Toronto/vaegen/venv/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py:171\u001B[0m, in \u001B[0;36m_BatchNorm.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    164\u001B[0m     bn_training \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrunning_mean \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrunning_var \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    166\u001B[0m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    167\u001B[0m \u001B[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001B[39;00m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001B[39;00m\n\u001B[1;32m    169\u001B[0m \u001B[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001B[39;00m\n\u001B[1;32m    170\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m--> 171\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_norm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    172\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    173\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001B[39;49;00m\n\u001B[1;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrunning_mean\u001B[49m\n\u001B[1;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrack_running_stats\u001B[49m\n\u001B[1;32m    176\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    177\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrunning_var\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrack_running_stats\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    178\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    179\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    180\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbn_training\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    181\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexponential_average_factor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    182\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    183\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Workspace/Education/University/Toronto/vaegen/venv/lib/python3.9/site-packages/torch/nn/functional.py:2448\u001B[0m, in \u001B[0;36mbatch_norm\u001B[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001B[0m\n\u001B[1;32m   2435\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m   2436\u001B[0m         batch_norm,\n\u001B[1;32m   2437\u001B[0m         (\u001B[38;5;28minput\u001B[39m, running_mean, running_var, weight, bias),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2445\u001B[0m         eps\u001B[38;5;241m=\u001B[39meps,\n\u001B[1;32m   2446\u001B[0m     )\n\u001B[1;32m   2447\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m training:\n\u001B[0;32m-> 2448\u001B[0m     \u001B[43m_verify_batch_size\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2450\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mbatch_norm(\n\u001B[1;32m   2451\u001B[0m     \u001B[38;5;28minput\u001B[39m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001B[38;5;241m.\u001B[39mbackends\u001B[38;5;241m.\u001B[39mcudnn\u001B[38;5;241m.\u001B[39menabled\n\u001B[1;32m   2452\u001B[0m )\n",
      "File \u001B[0;32m~/Workspace/Education/University/Toronto/vaegen/venv/lib/python3.9/site-packages/torch/nn/functional.py:2416\u001B[0m, in \u001B[0;36m_verify_batch_size\u001B[0;34m(size)\u001B[0m\n\u001B[1;32m   2414\u001B[0m     size_prods \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m=\u001B[39m size[i \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m2\u001B[39m]\n\u001B[1;32m   2415\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_prods \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m-> 2416\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected more than 1 value per channel when training, got input size \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(size))\n",
      "\u001B[0;31mValueError\u001B[0m: Expected more than 1 value per channel when training, got input size torch.Size([1, 200])"
     ]
    }
   ],
   "source": [
    "model_directory = os.path.join(\"models\", \"subsets_test\")\n",
    "\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "\n",
    "n_steps = len(subsets_adata_by_cell_type)\n",
    "\n",
    "df_results = pd.DataFrame(data=np.zeros((n_steps, 2)), index=np.arange(n_steps), columns=['r2', 'r2_diff_genes'])\n",
    "\n",
    "for i, subset_adata_by_cell_type in enumerate(subsets_adata_by_cell_type):\n",
    "    adata_sample = ad.concat(list(subset_adata_by_cell_type), join=\"outer\")\n",
    "    params_filename = os.path.join(model_directory, f\"{i}_autoencoder.pt\")\n",
    "    create_and_train_vae_model(adata_sample,\n",
    "                               epochs=15,\n",
    "                               save_params_to_filename=params_filename)\n",
    "    r2, r2_diff_genes = evaluate_r2(params_filename)\n",
    "    df_results.loc[i, [\"r2\", \"r2_diff_genes\"]] = [r2, r2_diff_genes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.plot(y=['r2', 'r2_diff_genes'], kind='line');\n",
    "plt.title('Impact of cell type subsetting on model score')\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('$R^2$');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
